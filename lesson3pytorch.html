<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="icon" sizes="180x180" href="android-chrome-512x512.png">

    <title>Importing Data: Pytorch Lesson 3</title>
    <style>
        #title {
            text-align: center;
            font-size: 64px;
        }
        body {
            font-family: Calibri;
        }
        #text {
            font-size: 200%;
            width: 95%;
            text-align: center;
            margin-left: auto;
            margin-right: auto;
        }
        .sub {
            font-size: 40px; 
            text-align:left;
        }
    </style>
</head>
<body>
    <h1><a href="https://kukuquack.github.io/home/" style="text-decoration: none; color: black;">Omar Zayed | Deep Learning</a></h1>
    <h1 id="title">Importing Data: Pytorch Lesson 3</h1>
    <em><p style="text-align: center;">28 Aug, 2021</p></em>
    <em><p style="text-align: center;"></p></em>
    <div id="text">
        <p>Data is the most important thing when making a machine learning/deep learning model, because how are we supposed to learn? We use the data that we are provided in a task to train and evaluate the model. Data can be found all over the internet. You can get data by scraping websites, finding datasets on websites like <a href="https://www.kaggle.com">kaggle</a>, which is the most famous website that provides data and hosts machine learning competitions. You can also be provided data as a spreadsheet, though you might have to do some cleaning using tools provided by 3rd party libraries in python. Although data is everywhere, it doesn't mean that all of it is beneficial. Some data maybe unlabeled, and in this case it'd be useless (except if you have days to label it yourself of course ðŸ˜›).</p>
        <h1 class="sub">Data In Pytorch</h1>
        <p>In Pytorch, we can either import data from our system (if already downloaded), or we can import standard data sets that is provided by Pytorch. These include the famous <a href="https://pytorch.org/vision/stable/datasets.html#mnist">MNIST</a> data set which is used for handwritten digit recognition.</p>
        <h1 class="sub">Importing MNIST</h1>
        <img src="p4_1.png">
        <p>First, we import MNIST from <code>torchvision.datasets</code>. You can find a lot of data sets there, though our focus now is on MNIST. Then we call an instance of <code>MNIST</code>. We set where we want our data to be saved by setting <code>root='data/'</code>. We set <code>download</code> to <code>True</code> so that we can download the data set. We also set <code>transform</code> to <code>ToTensor()</code> which will transform the image data to tensors, which we talked about in the first lesson which you can visit <a href="https://kukuquack.github.io/home/lesson1pytorch.html">here</a>. Data in Pytorch needs to be converted into tensors as it is what Pytorch uses. You can't use normal image files. There are also other types of transformations that can be applied (they can be joined together too).</p>
        <img src="p4_2.PNG">
        <p>After we have imported the data, we need to divide it into 2, the training set and the validation set. Here the validation set is of size 10000 and the training set is 60000. This is the data set (not including the test set) is comprised of 60000 images.</p>
        <img src="p4_3.png">
        <p>Here we put our data in <code>DataLoaders</code>. They are iterables that divide our data into batches when you iterate over them. We have batches so that the memory can fit in memory, as we can't fit all these images all at once! We have the batch size in the validation set double the batch size in the training set. This is because we don't need to do any learning which takes up memory.<code> shuffle=True</code> allows us to have differently generated batches each epoch. This shuffling allows the model to generalized quicker. The validation loader doesn't need to be shuffled as it is only used for validation.</p>
        <img src="p4_4.png">
        <p>Now we import the training data and make a <code>DataLoader</code> out of it. The <code>MNIST</code> class has train set to <code>False</code>, meaning we want the other part of the data that has been put aside for testing.</p>
        <h1 class="sub">Importing Tabular Data</h1>
        <p>To import tabular data and make a model for it in Pytorch, you can first import the data and load it using numpy, then you can change the numpy array into a tensor using the <code>torch.from_numpy()</code> function. Then put the inputs and targets in a <code>TensorDataset</code>. Then you split the data set into a training and test set using <code>random_split()</code>, then create a <code>DataLoader</code> for each partition.</p>
        <img src="p4_5.png">
        <h1 class="sub">Importing Image Data</h1>
        <img src="p4_6.png">
        <p>There are many folder structures when downloading image data on the internet. This one can be seen quite often. If your data set is divided into a training set and test set, and each class has its own folder, then you can import the data as follows:</p>
        <img src="p4_7.png">
        <p>We create an <code>ImageFolder</code> with the folder of the training data and apply a <code>ToTensor()</code> transformation, then we split the data into a training set and a validation set using <code>random_split()</code>. We then set the <code>batch_size</code> to some number, then we create a <code>DataLoader</code> for each partition.</p>
        <img src="p4_8.png">
        <p>We create an <code>ImageFolder</code> for the test folder, and apply the same transformation, then we create a <code>DataLoader</code>.</p>
        <h1 class="sub">Data Augmentation/Normalization</h1>
        <p>We decided to make this part separate from the code above so that you can grasp everything. Data augmentation is very important in deep learning. They are random transformations that get applied to the data randomly. This helps the model to be more generalized as it doesn't see the same images every epoch.</p>
        <img src="p4_10.PNG">
        <p>Here we are composing some transforms that will be applied to the training data using <code>tt.Compose</code>. There are a lot of transforms that you can get with Pytorch, here we have a few. You can search the internet for some that you might want to apply.</p>
        <p>Data normalization is also plays a very important role. It keeps the input into a smaller range than before. It helps the model to converge quickly as the weights don't need to keep changing due to the difference in ranges. It also prevents the values from any channel from unfairly affecting the gradients while training because it has a bigger range of values than other channels. It isn't necessary however, but it will be required when the inputs have different ranges. Do keep that in mind!</p>
        <p>We normalize the data by subtracting the mean and dividing by the standard deviation across each channel.</p>
        <img src="https://i.imgur.com/LYxXBVg.png" width="500px">
        <p>This is how we normalize the data:</p>
        <img src="p4_9.PNG">
        <p>First I define the mean and standard deviation that I want to use, then I compose a few transformations to be applied to the data (in this case I only changed the data to tensors), then I normalize the data using <code>stats</code>. We put an asterisk here so that we don't need to manually put each element of <code>stats</code> as an argument. We set <code>inplace</code> to <code>True</code> so that we change the input directly rather than making copies of the data which saves memory.</p>
        <h1 class="sub">Conclusion</h1>
        <p>So we were able to import data from our computer using Pytorch. In the next lessons we will use this data to train a model on it. See you next lesson!</p>
    </div>
</body>
</html>